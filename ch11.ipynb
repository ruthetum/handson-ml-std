{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFa6ZcLgxMX3sZqIUAZY8L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruthetum/handson-ml-std/blob/master/ch11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ml25uaDGok"
      },
      "source": [
        "# Chapter 11 심층 신경망 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl7eHZJsOF0u"
      },
      "source": [
        "# Settings\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "import numpy as np\n",
        "import os\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvcQGButRtQ7"
      },
      "source": [
        "## 훈련 중 문제 상황\n",
        "- 기울기 소실/폭주\n",
        "- 훈련 데이터 부족\n",
        "- 대규모 모델의 훈련 속도 저하"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J52aTudHc50W"
      },
      "source": [
        "## 1. 기울기 소실/폭주 해결"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-poItA0MDggv"
      },
      "source": [
        "문제 : 로지스틱 시그모이드 활성화 함수와 가중치 초기화 방법\n",
        "- 로지스틱 활성화 함수에 의해 역전파가 진행될 때 최상위층에서부터 전달되는 그레디언트가 실제 아래층에는 도달하지 않게 됨\n",
        "\n",
        "### 1) 글로럿 초기화 & He 초기화\n",
        "- 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 그레디언트를 보장\n",
        "- 그 대안으로 각 층의 연결 가중치를 무작위로 초기화\n",
        "\n",
        "  **(1) 글로럿 초기화**\n",
        "  ```\n",
        "  keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "  ```\n",
        "\n",
        "  **(2) He 초기화**\n",
        "  ```\n",
        "  init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)\n",
        "  ```\n",
        "\n",
        "### 2) 수렴하지 않는 활성화 함수\n",
        "- 활성화 함수를 잘못 선택하면 그레디언트의 소실이나 폭주로 이어짐\n",
        "- ReLU는 계산이 빠르지만 완벽하지 않음 (ex. dying ReLU 문제)\n",
        "\n",
        "  **(1) LeakyReLU**\n",
        "  ```\n",
        "  keras.layers.LeakyReLU() # alpha default : 0.3\n",
        "  # or\n",
        "  # keras.layers.LeakyReLU(alpha=0.2)\n",
        "  ```\n",
        "  **(2) PReLU**\n",
        "  ```\n",
        "  keras.layers.PReLU()\n",
        "  ```\n",
        "  **(3) RReLU**\n",
        "  ```\n",
        "  # 케라스 공식문서에 RReLU 구현은 아직 없음\n",
        "  ```\n",
        "  **(4) SELU**\n",
        "\n",
        "  Günter Klambauer, Thomas Unterthiner, Andreas Mayr는 2017년 한 훌륭한 논문에서 SELU 활성화 함수를 소개했습니다. 훈련하는 동안 완전 연결 층만 쌓아서 신경망을 만들고 SELU 활성화 함수와 LeCun 초기화를 사용한다면 자기 정규화됩니다. 각 층의 출력이 평균과 표준편차를 보존하는 경향이 있습니다. 이는 그레이디언트 소실과 폭주 문제를 막아줍니다. 그 결과로 SELU 활성화 함수는 이런 종류의 네트워크(특히 아주 깊은 네트워크)에서 다른 활성화 함수보다 뛰어난 성능을 종종 냅니다. 하지만 SELU 활성화 함수의 자기 정규화 특징은 쉽게 깨집니다. ℓ1나 ℓ2 정규화, 드롭아웃, 맥스 노름, 스킵 연결이나 시퀀셜하지 않은 다른 토폴로지를 사용할 수 없습니다(즉 순환 신경망은 자기 정규화되지 않습니다). 하지만 실전에서 시퀀셜 CNN과 잘 동작합니다. 자기 정규화가 깨지면 SELU가 다른 활성화 함수보다 더 나은 성능을 내지 않을 것입니다.\n",
        "\n",
        "  [출처] : https://github.com/rickiepark/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb\n",
        "  ```\n",
        "  keras.layers.Dense(10, activation=\"selu\",  kernel_initializer=\"lecun_normal\")\n",
        "  ```\n",
        "\n",
        "  \\+ Difference between ELU and SELU : z<0에서 곡선의 차이가 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX6Zxr3qFkFx"
      },
      "source": [
        "#### cf. 심층 신경망의 은닉층에는 어떤 활성화 함수를 써야할까.\n",
        "- 보통 **SELU > ELU > LeakyReLU(와 ReLU의 변형들) > ReLU > tanh > 로지스틱** 순으로\n",
        "- 네트워크가 자기 정규화되지 못 하는 구조 : ELU > SELU 일 수도. 속도가 중요하다면 LeakyReLU도 괜찮\n",
        "- 신경망이 과대적합됐다면 RReLU\n",
        "- 훈련 세트가 아주 크다면 PReLU\n",
        "- 속도가 가장 중요하다면 ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4iNqkzpKQMD"
      },
      "source": [
        "### 3) Batch Normalization (배치 정규화)\n",
        " - ELU와 함께 He 초기화를 사용하면 훈련 초기 단계에서 그레디언트 소실/폭주 문제를 감소시킬 수 있지만, 훈련하는 동안 다시 발생할 수 있음 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaZFyoZ7EaAS",
        "outputId": "1438c197-bdcb-4a6c-f800-bb108847f048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\" , kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8hraBLePGPx",
        "outputId": "aca4c511-4f5d-42be-cd0d-f0ba1711eb7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6HBJv0HPVfb",
        "outputId": "3bab6521-6e6a-463c-b7b3-beccfd8481b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "model.layers[1].updates"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-11fe563bf3e2>:1: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ocTKDKPqN0"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300,  kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8viFxy34Qw46"
      },
      "source": [
        "### 4) Gradient Clipping (그레디언트 클리핑)\n",
        " - 역전파될 때 그레이언트의 임곗값을 설정하여 넘어서는 값은 제거\n",
        "```\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yi1E06qc21b"
      },
      "source": [
        "## 2. 사전훈련된 층 재사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHgrE6zMRcWq"
      },
      "source": [
        "### 1) 전이학습\n",
        "- 비슷한 유형의 문제를 처리한 신경망을 찾아보고, 해당 신경망의 하위층을 재사용\n",
        "```\n",
        "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "model_B_on_A = keras.models.Sequential(model_A.layer[:-1])\n",
        "model_B_on_A = keras.models.load_model(1, activation=\"sigmoid\"))\n",
        "```\n",
        "- 이 경우 model_B_on_A가 훈련할 때 A도 영향을 받음 (층을 공유하는 상황이기 때문)\n",
        "- 영향을 원하지 않는 경우 복제해서 가중치를 복사\n",
        "- 먼저 원래 모델 복제 후 가중치 저장\n",
        "``` \n",
        "model_A_clone = keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "```\n",
        "- 새로운 층에게 가중치를 학습\n",
        "```\n",
        "for layer in model_B_on_A.layer[:-1]:\n",
        "    layer.trainable = False # 기존 가중치를 변형하지 않기 위해 false\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "```\n",
        "- 이후 다시 컴파일\n",
        "```\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "optimizer=keras.optimizers.SGD(lr=1e-4) # default 1e-2\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTdBHaocV2Tn"
      },
      "source": [
        "### 2) 비지도 사전훈련\n",
        "- 레이블된 훈련 데이터가 많지 않은 경우, 비슷한 작업에 대해 훈련된 모델을 찾을 수 없는 경우\n",
        "- 오토인코더, GNA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tjUjec2c_Lh"
      },
      "source": [
        "## 3. 고속 옵티마이저"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg5Fu2CLXpxf"
      },
      "source": [
        "- 경사 하강법 대신 더 빠른 옵티마이저를 사용\n",
        "\n",
        "(1) 모멘텀 최적화\n",
        "\n",
        "(2) 네스테로프 가속 경사\n",
        "\n",
        "(3) AdaGrad\n",
        "\n",
        "(4) RMSProp\n",
        "\n",
        "(5) Adam, Nadam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqVX5o71YYPN"
      },
      "source": [
        "### (1) 모멘텀 최적화\n",
        "```\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "```\n",
        "### (2) 네스테로프 가속 경사\n",
        "```\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
        "```\n",
        "### (3) AdaGrad\n",
        "```\n",
        "optimizer = keras.optimizers.Adagrad(lr=0.001)\n",
        "# 간단한 2차 방정식 문제에 대해서는 잘 작동하지만 신경망을 훈련할 때 종종 너무 일찍 멈춤\n",
        "# 따라서 심층 신경망에서는 사용하지 않는 게 좋음\n",
        "```\n",
        "### (4) RMSProp\n",
        "```\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
        "```\n",
        "### (5) Adam\n",
        "```\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "```\n",
        "### (5)-(1) AdaMax\n",
        "```\n",
        "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "# Adam이 잘 작동하지 않을 때 사용해볼만 함\n",
        "```\n",
        "### (5)-(2) Nadam\n",
        "```\n",
        "# Adam + 네스테로프\n",
        "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "```\n",
        "### (6) 학습률 스케줄링\n",
        "- 거듭제곱 기반 스케줄링\n",
        "- 지수 기반 스케줄링\n",
        "- 구간별 고정 스케줄링\n",
        "- 성능 기반 스케줄링\n",
        "- 1 사이클 스케줄링\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRImXO9JaaIS"
      },
      "source": [
        "## 4. 규제를 사용해 과대적합 피하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi8c_GWbxQf"
      },
      "source": [
        "### 1) L1, L2 규제\n",
        "- L1 : (많은 가중치가 0인) 희소 모델을 만들기 위해 사용\n",
        "```\n",
        "kernel_regularizer=keras.regularizers.l1(0.01)\n",
        "```\n",
        "- L2 : 신경망의 연결 가중치를 제한하기 위해 사용\n",
        "```\n",
        "kernel_regularizer=keras.regularizers.l2(0.01)\n",
        "```\n",
        "- Both L1 and L2\n",
        "```\n",
        "kernel_regularizer=keras.regularizers.l1_l2(0.01)\n",
        "```\n",
        "\n",
        "### 2) Dropout (드롭아웃)\n",
        "```\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "```\n",
        "\n",
        "### 3) 몬테 카를로 드롭아웃\n",
        "```\n",
        "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
        "y_proba = y_probas.mean(axis=0)\n",
        "np.round(model.predict(X_test_scaled[:1]), 2)\n",
        "```\n",
        "\n",
        "### 4) 맥스-노름 규제\n",
        "```\n",
        "ayer = keras.layers.Dense(100, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_constraint=keras.constraints.max_norm(1.))\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ]
}